Redis集群：主从复制、CAP、PAXS--2

问题：当数据量很大时，怎么往redis中放？

一、方案一：从访问redis的客户端入手：

（1）根据业务逻辑拆分：客户端对数据根据业务进行逻辑拆分，将不同的数据存放在不同的redis中

但有些时候，数据根据业务逻辑是没发拆分的，那么这个时候我们就有下面这种方案的产生


（2）根据算法进行拆分，如根据hash+取模进行拆分（官方名次叫：modula）
注意，上面的这些方式，就是对数据进行分片了，因为每个redis片上的数据，合在一起，才算是完整的数据
但是这种方式，有一个弊端：
这个弊端就是取模的数必须是固定的。会影响在分布式环境下的扩展性。
比如咱们redis初始有3台，我们对3取模，然后数据进行分片存储，但是随着业务量的增大，有可能3台redis已经不满足咱们需求了，就需要增大redis的个数，比如增至
5台，这个时候，我们对5取模，这个取模后的值就会发生变化，让原本应当到2这台机器上取的数据，结果对5取模后，到3这台机器上去取了，这样就会导致取不到对应的数据。
这就影响了在分布式环境下的扩展性。

对于取模是一个固定数的这个问题，可以通过random（随机数）来解决，如下方案：

（3）算法：Random
即往多台redis中存值的时候，通过随机数计算，随机将结果存入某一台redis中
但是这个有个问题，因为你是通过随机数往里面仍的，所以你在取的时候，也不知道该从哪台redis中取，所以这种模型，是有固定场景的：消息队列
比如我们有3台redis,都存储了一个key，叫lfw,并且这个key是一个list,我们往里面存值的时候，通过lpush存进去
在另外一方，需要取到对应的值，因为3台机器中都有这个值，那么我们只需要轮询3台机器，就可以将值取出，这就类似与消息队列，其特点是基于内存，速度快，但不可重复消费；这种有点想kafka，kafka
与其的区别就是基于磁盘，可以重复消费。

解决第二种方法（根据hash+取模进行拆分）的问题，也是有解决方案的：
（4）算法：kemata(一致性hash)
它的原理是不对redis的台数取模，而是对data以及node(一个redis看作一个node)进行hash运算，并将运算出来的值规划成为一个环形。
我们对node进行hash运算，比如有3个节点，进行hash运算后，根据hash值的大小，将其看作为一个环形。
一个data(key)进入，也对其进行hash运算，会计算出一个hash值，有可能这个hash值刚好等于之前咱们计算出node的hash值，那么就直接存储，也有可能未能命中，那么我们就将这个data存入离其最近（往后找）的一个hash值，
对应的node上。通过data(key)取值时，方式类似
但是这种方式也会有一个问题，如果我们新增了node，那么其计算出的hash值，与其前一个node的hash值之间的区域，如果已经存在相应的data（key）存储对应的node中了，那么再次取这个值的时候，就可能会取不到值了

所以，这种方式也有相应的缺点，这个问题也有对应的解决方案：
取值时，取最近的两个节点的值（还是可能会取不到，但是取的节点越多，设计会越复杂）

这个问题，也会造成缓存击穿，将压力压倒mysql上
也可以设计节点的淘汰算法，来更新缓存的信息，但是一样会增大设计复杂性
注意：这种设计方式，更倾向于用作缓存，而不是用作数据库！！！因为他在增删节点时，会造成数据时间差的丢失不可用

另外，我们可以通过在这个环上，增加虚拟节点，来解决数据倾斜（就是全压在一台节点上）的问题。在计算node对应的hash值时，给node+10，或者node+5，让node分布得更加宽泛，这样就可以解决数据倾斜的问题。


对于客户端的链接，通常情况下，咱们的客户端会比较多，每个客户端都需要链接到redis上，如果连接redis的客户端比较多，就会给redis服务器造成的一定的压力（连接的成本是很高的）
那么这个问题怎么解决呢？
对于这个问题，就涉及到了架构的演变和升级
首先，由于过多的客户端连接redis服务器，会给redis服务器造成相应的压力，那么我们就可以很自然的想到，可以在redis服务器前面在套一层nginx,做反向代理
对于nginx也可以做集群，每台nginx管理固定量的redis客户端，用于减轻连接代理压力；
注意，代理层也是有相应的逻辑和算法的，具体的算法这些，见上。这些算法都是无状态的，无状态是指，他不存数据，数据都是存在redis上的，他只管对数据的处理和筛选，只有无状态，才能进行nginx的集群扩展。
其次，如果nginx也撑不住了怎么办？
这个时候，我们就可以在nginx前面再套一层，lvs
lvs对应VIP，对于客户端而言，他们对于后面的操作都是透明的，所以对于他们而言，都是一个固定的IP地址，不论你后面的怎么变，对于这些客户端而言，就是一个固定的IP地址
学习地址：github:twemproxy

到此，以上所说的三种客户端算法处理方式，除了他们本身各自缺点外，统一的缺点，就是只能用来做缓存，没法用来做分布式数据库用

上面介绍的三种方法，都各自有各自的缺点，处理问题最好的，就是一致性hash，但是他也有问题，比如，数据在需要迁移时，需要重新根据节点做一次rehash，这个成本是非常高的，在这段时间差范围内，就有可能
导致服务不可用或者获取不到实际想要的值。
那么有没有一种办法，可以解决这些问题呢？
有，下面介绍预分区：

那么什么是预分区呢？

比如我们假设有10个槽位或者说有10个分片，开始这10个槽位位于2个redis上，每个redis上平均分了5个槽位，redis上，还有一个mapping，这个mapping就具体槽位分配与redis的映射。
后续随着业务量的增加，我们新增了一个redis，此时，我们假设将第一个redis上的2个槽位1，2；第二个redis上的2个槽位8，9；分配给了新增的redis，那么此时新增的redis上的mapping映射就有1，2，8，9
这4个槽位，同时，将对应的槽位的数据迁移至新增的redis中即可
这样，就避免了rehash的过程，也避免了在时差时，数据不一致或服务不可用的问题。

那么redis集群是怎么做的呢？
redis的分片方式中，没有主，这些集群中的redis，分领全部槽位的1/n，客户端想连哪个redis服务器就可以哪个redis服务器，比如客户端连接第二个redis,想要取k1对应的值，
但这个数据存放在第三个redis的4号槽位上。
此时，在第二个redis中，进行一次hash运算，运算出当前key位于哪个槽位上，和对应的mapping做一个比对， 如果自己没有，那么就看其位于哪个槽位上，每个槽位与对应redis的映射关系，
在每个redis中也是存在的。获取到槽位及对应redis映射关系后，将对应redis的所在位置编号，返回给客户端，客户端再重定向到返回的所属redis编号的服务器上，再直接到对应槽位上取值。

注意，客户端重定向到redis3上后，redis3中还需要重新对key进行hash运算，算出其对应槽位，然后再到对应槽位上取值。

也就是说，对于key的hash运算的功能，以及其他每个槽位与其对应redis的映射关系，是处于每个redis服务中的。

注意：
数据分治，必然会存在一个问题：聚合操作很难实现
比如对两个不同槽位上的数据进行取交集这种操作，就很难实现了；再比如，在不同槽位上的数据，你要控制事物，也是难以实现的

但是，redis对于这样分治的数据，也提供了对应的解决方案：hash tag
怎么理解呢？我们知道，之所以这些聚合操作很难实现，是因为这些数据位于不同的槽位上，那么我们只需要想办法，让这些我们想要进行聚合处理的数据，放到一个槽位上，不就可以完美解决这个问题了吗？
是的，hash tag就是这么干的。
比如，我们定义两个key：
第一个key，他的key名为：{ooxx}k1
第二个key，他的key名为：{ooxx}k2
我们进行取模运算的时候，不要再对他们不同的key名，也就是k1或者k2进行hash运算了，我们对其相同的部分ooxx进行hash运算，由于他们的ooxx是一致的，因此，这两个key对应的数据肯定位于同一槽位上。
这样，我们就可以实现数据的聚合处理了。

附：redis集群有16384个hash槽

后续的实操，在有时间的情况下，可以多看看

使用代理的目的，就是解耦客户端与服务端



 









