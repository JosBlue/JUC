zk原理知识、paxos zab协议
本节主要学习的知识点包括：
1、paxos
2、zab协议
3、watch
4、API（让可以编写zk client）
5、callback --> reactive 响应式编程
响应式编程，可以更加充分的压榨OS,HW资源，提升性能


一、paxos
首先，对于zk我们要有一个基本的认识，他是干什么的？
zk可以做分布式协调，它具有高扩展性，可靠性，时序性，以及十分快速
（zk的快不仅体现在查询快，同时，当leader挂掉以后，由不可用状态切换至可用状态也十分快）

1、扩展性
首先，他的扩展性，基于他的架构（框架）设计，分为不同的角色，包括leader,follower以及Observer，读写分离
Leader:集群服务中，只有一个leader，可执行读写请求

Follower：集群服务中，大多数为follower，可执行读请求，所有的写请求都被转发到leader上进行执行；
当leader挂掉后，follower会参与投票选出新的leader

Observer：集群中，还有部分为Observer，Observer只可执行读请求，当Leader挂掉之后，Observer不会参与选举投票
为什么Observer不参与投票，可以设想，从3个人中选举出Leader容易还是从10个人中选举出Leader容易这个问题来思考
所以，设置Observer的目的，是为了放大查询能力，同时，在选举新的Leader时，减少Follower的数量，也能更加快速的选举出新的Leader

那么怎么配置Observer呢？
回忆我们之前提到的zoo.cfg配置文件，假设我们有4台机器，那么我们的配置文件的格式就是：
server.1=node01:2888:3888
server.2=node02:2888:3888
server.3=node03:2888:3888
server.4=node04:2888:3888

我们将第4台机器配置为Observer，那么配置文件就修改为：
server.1=node01:2888:3888
server.2=node02:2888:3888
server.3=node03:2888:3888
server.4=node04:2888:3888:observer

这样第4台机器就被配置为Observer了
在我们的实际工作场景中，比如我们有70台机器，我们实际上只需要将follower配置为个位数，并且满足过半选举就可以了，这样可以最大化
放大查询效率
比如我们留5台Follower，过半选举的数量为3台，允许服务挂掉的数量为follower数量的一半减一，也就是允许2台服务挂掉，因为还剩下3台，
也可以选举出Leader
5台服务同时挂掉的这种情况，在实际生产环境中几乎不会出现同时宕机的情况，所以这样配置是合理的。


2、可靠性
关于可靠性，可以记住一句话：攘其外必先安其内
怎么理解呢？zk的可靠性，来源于当zk集群的leader挂掉以后，可以快速选举出新的leader。
将zk集群比作一个国家，一个国家想要稳定发展，就需要有一个稳定的国内环境，同时有一个稳定的国家领导人，当国家领导人进行换届选举，或出现意外时，能够快速选举出新的，有能力的国家领导人，这样才能
维持一个国家的良好发展，从而更好的对外提供各种服务。
攘其外：就是指对外提供稳定的服务
安其内：就是指快速恢复Leader
因此，换回zk集群来看，zk集群的可靠性就是源于能够快速恢复Leader

那么快速恢复leader这个是怎么实现的呢？


可靠性，除了表现在对外提供服务的可靠，还表现在其数据的可靠，可用以及一致性的特性上。这个属于攘其外的部分
那么数据的一致性属于什么一致性呢？
肯定是最终一致性！那么最终一致性是怎么实现的？
但是在最终一致性的过程中，存在一些还没同步完数据的节点，那么这部分节点是否要对外提供服务呢？
实际上，在zk集群中，这些还没同步完数据的节点，如果对外提供服务，那么就会出现脑裂的情况，所以那些没有主键过半数的节点，最终是会shutDown自己的服务的？？
这样也是zk的一个特色

关于数据的可靠性，一致性的这个问题，这里是在分布式的环境下要实现的，在分布式环境下要实现这些，实际上是十分复杂的。
这个就要涉及到一个协议了，paxos协议

什么是Paxos？(https://www.douban.com/note/208430424/)
Paxos是一种基于消息传递的一致性算法（1990年提出）
Paxos的前提：没有拜占庭将军问题，即当前的计算机环境是可信的
即在一个可信的计算机环境中，一种基于信息传递的一致性算法

Paxos描述了这样一个场景，有一个叫做Paxos的小岛(Island)上面住了一批居民，岛上面所有的事情由一些特殊的人决定，他们叫做议员(Senator)。
议员的总数(Senator Count)是确定的，不能更改。
岛上每次环境事务的变更都需要通过一个提议(Proposal)，每个提议都有一个编号(PID)，这个编号是一直增长的，不能倒退。
每个提议都需要超过半数((Senator Count)/2 +1)的议员同意才能生效。每个议员只会同意大于当前编号的提议，包括已生效的和未生效的。
如果议员收到小于等于当前编号的提议，他会拒绝，并告知对方：你的提议已经有人提过了。
这里的当前编号是每个议员在自己记事本上面记录的编号，他不断更新这个编号。整个议会不能保证所有议员记事本上的编号总是相同的。
现在议会有一个目标：保证所有的议员对于提议都能达成一致的看法。

好，现在议会开始运作，所有议员一开始记事本上面记录的编号都是0。有一个议员发了一个提议：将电费设定为1元/度。
他首先看了一下记事本，嗯，当前提议编号是0，那么我的这个提议的编号就是1，于是他给所有议员发消息：1号提议，设定电费1元/度。
其他议员收到消息以后查了一下记事本，哦，当前提议编号是0，这个提议可接受，于是他记录下这个提议并回复：我接受你的1号提议，同时他在记事本上记录：当前提议编号为1。
发起提议的议员收到了超过半数的回复，立即给所有人发通知：1号提议生效！收到的议员会修改他的记事本，将1好提议由记录改成正式的法令，当有人问他电费为多少时，他会查看法令并告诉对方：1元/度。

现在看冲突的解决：假设总共有三个议员S1-S3，S1和S2同时发起了一个提议:1号提议，设定电费。
S1想设为1元/度, S2想设为2元/度。结果S3先收到了S1的提议，于是他做了和前面同样的操作。
紧接着他又收到了S2的提议，结果他一查记事本，咦，这个提议的编号小于等于我的当前编号1，于是他拒绝了这个提议：对不起，这个提议先前提过了。
于是S2的提议被拒绝，S1正式发布了提议: 1号提议生效。S2向S1或者S3打听并更新了1号法令的内容，然后他可以选择继续发起2号提议。


在这个故事中，过半投票的这个行为，实际上隐藏了一个两阶段提交的思想过程
收到的提议，先写入日志，保证需要操作事情的可靠性的支撑，然后等收到过半通知时，再开放当前这件事，或者说这个数据才能被外部查询

二、ZAB协议（Zookeeper 原子广播协议）：
原子：要么成功，要么失败，不存在中间状态（队列+2PC保证）
广播：分布式多节点，只要过半收到通知即可 

ZAB协议是paxos算法的精简版，更容易实现数据在可用状态（有leader）下的数据同步。
注：zk的数据状态保存在内存中，日志信息记录在磁盘上


ZAB协议核心图：
![avatar](/Users/liufuwei/Documents/my-project/my-juc/JUC/myJuc/image/ZAB协议.png) 
ZAB协议的核心：队列（FIFO） + 2PC（2阶段提交）  
图解：  
（1）客户端发起创建节点的请求  
（2）follower接收到这个请求，由于是写请求，所以必须提交给leader来进行处理，因此，follower将这个写请求提交给leader处理。
    leader接收到follower创建新节点的请求后，创建对应的事物id（事物id是单调递增的方式创建的）  
leader创建完事物id后，leader将会将创建的信息推送给所有的follower，其中包含了两阶段提交的思想    
（3）leader将创建新节点的请求，放进所有follower对应的队列中,此时，还未创建新的节点，只是在磁盘上记录了对应的日志信息     
（4）这些队列分别将信息发送给其对应的follower，follower记录创建节点的日志信息，follower记录完日志信息后，回送确认消息给leader       
（5）当回送节点加上leader节点，总节点数已经过半时，这个新增节点的请求就可以正式写了。此时，leader再通过对应的队列发送写（write）操作的指令（写到内存中）  
（6）推送的写请求，会将这个请求推送到所有follower的队列中，因此，即使那些在过半节点都已经回送确认消息后，可能由于网络波动、延迟等情况，而导致未返回确认节点，也会接收到对应写请求操作  
这样的处理，也保证了数据的最终一致性  
（7）写完后，follower也会回送确认消息，leader则回送确认消息给之前请求到leader的follower，follower则将确认消息回送给客户端  

如果在follower还未同步到leader的最新信息时，若有请求已经请求到这个follower上了，为了保证数据的一致性，可以让follower先同步完leader消息后，在处理对应请求。
但是这么做，会降低zk的可用性。由于zk保障的数据是最终一致性，因此，根据自身系统对数据一致性的要求高低，对是否同步进行取舍。

zookeeper一挂，所有人都对外停止提供服务，开始选leader，选出leader后，建立连接，同步完数据，才开始对外提供服务，做到最终数据一致性。
在这个过程中，节点是否对外提供服务，取决于节点的状态。
有些状态是可以提供服务的，有些状态不能对外提供服务（例如，从连不上主）。
一个client连接上掉线的follower，可以调用该follower的回调callback服务，等待follower连接上leader，sync同步完以后，拿到数据，回调服务就可以返回数据。

如果一个写请求被请求到leader,leader还未同步到follower就挂了，那么这个写请求相关的信息，都会回滚掉
因为整个请求相对于客户端都还没有回送确认这一步，相对于客户端而言，就是整个系统没有这一步


zookeeper选举：采用谦让制，要使用到server自己的myid和事务的Zxid。
M：myid server.id， Z：Zxid事务id
leader选举时，达到过半开始选举，先看事务id，大的选谁，事务id一样的情况下，选myid大的。



zk中leader挂掉到恢复的详细过程：
首先场景分为如下两个场景：
（1）初次启动
初次启动时，整个集群是没有版本，没有历史状态的

（2）集群重启或leader挂了后的状态
如果是集群重启，或leader挂掉的启动，肯定是产生过历史数据的

其次，要明白，集群中，每个节点都有自己的myid，同时，还有一个事物id(Zxid),即数据存储到哪个版本了
另外，作为leader的条件，那么就需要数据最全，即Zxid最大，当Zxid一致时，就判断谁的myid更大，则谁就是leader
（注意，这个leader选举出的情况，一定有一个前置条件:即你见到的所有数据，都是通过集群过半通过的真实数据。
只要你的集群中，挂掉的节点，没有超过半数，那么在这个集群中，一定是有过半的节点的事物id，是最大的那个事物id）



如果是第一种场景：初次启动
![avatar](/Users/liufuwei/Documents/my-project/my-juc/JUC/myJuc/image/leader选举的过程01.png) 
则，初次启动时，假设一共有4个节点，由于是初次启动，所以每个节点的Zxid都是0，假设这4个节点的myid依次递增，那么如果是按照1，2，4的方式启动的，根据事物id一致时。比较myid的大小这样的规则，
由于启动到node04时，集群中的节点已经过半，则node04，会被选举成leader
第一种场景十分好理解，关键是第二种场景


第二种场景：集群重启或leader挂了后的状态
![avatar](/Users/liufuwei/Documents/my-project/my-juc/JUC/myJuc/image/leader选举的过程02.png) 
假设这个集群中，node04开始是leader,node01、node02以及node04节点他们的Zxid都是8,但是node03节点的Zxid=7


假设，leader挂掉了，那么此时整个集群中，只剩下了3个节点，node01、node02、node03
![avatar](/Users/liufuwei/Documents/my-project/my-juc/JUC/myJuc/image/leader选举的过程03.png)

![avatar](/Users/liufuwei/Documents/my-project/my-juc/JUC/myJuc/image/leader选举的过程04.png)
由于节点与节点之间，会有心跳检测，1S 1次，当检测到leader挂掉后，就会开始投票选举出新的leader
如果恰好是node03率先检测到leader（node04）挂掉了，那么node03就会开始进行投票选举
作为投票者，node03会先给自己投一票，并且将自己的myid以及事物id作为投票信息，投递给node01和node02
当node01以及node02作为收票者，接收到node03投递过来的信息时，比较事物id后，发现自己的事物id比node03的事物id更大，就将自己的这些信息
返回给node03,即当收票者发现投票者的信息更小时，就需要淘汰此票，并纠正投票的票选信息
因此，node02以及node01就会将信息广播出去，即自己作为投票者，向另外两个节点推送票选信息，推送的同时，触发给自己投一票
此时的票选信息为:(都是自己给自己投的)
node01：1票
node02：1票
node03：1票

当node03接收到node01或node02的信息后，对比发现他们的节点的事物id更大，假设先接收到node02的信息，则经过比较后，node03会纠正自己投票信息，
即将自己的node03+1,修改为node02+1，纠正完成后，node03再将node02的信息推送给node02，即将票投给node02,此时node02就收到了来自node03的投票
此时node02：2票（自己投给自己的 + node03投的）
node03：node02一张票
同理，node01接收到node02的，也会给node02投票，此时node02就是3张票了

注意，在收到投票者的投票信息后，经过比较后，还会有一个广播纠正投票信息的过程
比如，node01收到node02的投票信息，发现node02经过比较后，比自己大，那么node01会将node02的信息推送给node03以及node02,
由于node01已经给node02投过票了，因此，在node02中，node02的票数不变
但是在node03中，由于接收到node01发送过来的node02的信息，因此在node03中，node02节点的票数会加1

整体投票过程简化：
前面是当前节点，后面是当前节点将票投给了哪个节点
node03率先投票：
node03->node03

node01接收到node03的投票信息，比较后，淘汰node03的票，然后给自己投一票，并广播自己的信息给node03及node02,这两个节点接收到node01的信息后
在node01中的投票情况：
node01->node01
node03->node01

在node03中的投票情况：
node03->node01

node02接收到node03的投票信息，比较后，淘汰node03的票，然后给自己投一票，并广播自己的信息给node03及node01,这两个节点接收到node02的信息后
在node02节点中的投票情况：
node02->node02
node03->node02
node01->node02

在node03节点中的投票情况：
node02->node02
node03->node02

node01接收到node02的信息后，更改信息,重新将票投给了node02，并广播给node03
则node03中的投票情况：
node02->node02
node03->node02
node01->node02

同理，node03将自己更改后的投票信息广播给node01：
则node01中的投票情况：
node02->node02
node03->node02
node01->node02


其中，关键点，接收到其他节点的信息后，如果投票变更，则会将这个变更结果广播给其他节点
这个过程中的投票不重要，最重要的是，最终所有节点中的投票信息，都会变成一种状态，就是node02的票数为3
投票完成后，node02就升级为leader,其他节点为follower，并同步node02的信息
有这个投票信息后，节点只需要判断自己是否为被投选出来的主节点即可，是，则为leader，不是，则为follower同步leader的信息，这个过程是十分迅速的

对zk中leader选举过程进行描述：
（1）3888端口，造成两两通信
（2）只要任何人投票，都会触发那个准leader发起自己的投票
（3）整个投票过程是推选制：先比较Zxid，如果Zxid相同，再比较myid


实战操作记录：
在一个集群中，当一些节点挂掉后，剩下的节点不足以支撑节点过半选举投票（比如，4个节点的机器，最多允许挂掉2台机器），那么剩下的节点，将
不再对外提供服务，因为不能保证这些剩下的节点的数据是否完全，所以如果继续对外提供服务，就有可能导致客户端拿到的数据不一致。无法保证数据的一致性
但是这些没有挂掉的节点，服务进程还是存在的

看到这里，顺便提一句，相对于redis而言，在可靠性方面，zk肯定比redis强，但是在响应速度上，zk肯定是比不上redis的


三、watch 监控/观察
![avatar](/Users/liufuwei/Documents/my-project/my-juc/JUC/myJuc/image/zk-watch.png)
对于watch，怎么理解呢？
我们都知道，对于zk，他有几个特性
首先是统一视图，也就是说，客户端调用zk上任何一个节点，在一段时间内获取到信息都是一样的
其次，zk本身是一个目录树结构，比如有一个节点node，上面有个根路径是/ooxx
此时，有两个客户端，客户端1和客户端2，客户端1在节点上创建了一个a,/ooxx/a,客户端2需要获取/ooxx/a的相关信息
那么此时就形成了一种关系，就是客户端2获取的信息，依赖于客户端1，这个其实与zk是没有什么关系的
但是，如果客户端1挂了，客户端2通过zk获取到/ooxx/a的信息就会受到影响，有可能获取到的是一些错误的信息
zk是做分布式协调服务的，这种情况怎么去处理呢？
此时，就可以用到watch
客户端2获取信息后，在watch /ooxx/a的信息
如果客户端1是一个临时节点，那么当客户端1对/ooxx/a存在诸如创建、删除，修改或者是对其子目录进行操作时，zk就会产生一个事件，并且将这个事件
回调给客户端2，客户端2就可以针对这些事件做出相应的操作

当然，客户端2自己也可以做心跳，一秒钟或者3秒钟检测一次客户端1的状态，但是这样就会有一段时间的延迟
而zk的watch+callback则是非常迅速的反应，就能是及时的做出响应

总结：
对于观察，可以有两种实现方式
1、客户端自己实现心跳检测
2、使用基于zk的watch
那么这两者有什么区别呢？
本质上，是在方向性和时效性上
对于方向性：
客户端自己实现心跳检测，就是客户端2->客户端1，即从客户端到客户端
如果是基于zk的watch,那么方向就是客户端2->zk，客户端2就不需要自己在实现心跳检测了

对于时效性上：
客户端自己实现的心跳，会有一个心跳检测的时间差
但是基于zk 的watch，一旦产生了事件，zk就会callback，这个时效是非常迅速的

注意：watch不定于广播，callback只给watch的客户端发

四、API
注意：zk有session，但是没有线程池的概念
new zookeeper中的参数：
watch:有两类
第一类：new zk的时候，传入的watch，这个watch是session级别的，跟path，node没有关系
这个watch和node所有相关的事件都是收不到的，他可以监控事，就是zk连接的状态，比如这个连接断了，重新连了另外一个连接
测试类见：ZooKeeperTest





















  


















